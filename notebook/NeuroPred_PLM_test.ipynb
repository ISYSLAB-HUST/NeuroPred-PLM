{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### INSTALL"
      ],
      "metadata": {
        "id": "V4JeaQZ_Ghh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ2-qjClDfpV",
        "outputId": "eb27faa0-654b-40a4-a7b8-0d699b2069ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ISYSLAB-HUST/NeuroPred-PLM.git\n",
            "  Cloning https://github.com/ISYSLAB-HUST/NeuroPred-PLM.git to /tmp/pip-req-build-cxcsgbok\n",
            "  Running command git clone -q https://github.com/ISYSLAB-HUST/NeuroPred-PLM.git /tmp/pip-req-build-cxcsgbok\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-1.0.2-1-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from NeuroPredPLM==0.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from NeuroPredPLM==0.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->NeuroPredPLM==0.1) (4.1.1)\n",
            "Building wheels for collected packages: NeuroPredPLM\n",
            "  Building wheel for NeuroPredPLM (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NeuroPredPLM: filename=NeuroPredPLM-0.1-py3-none-any.whl size=7181 sha256=cf2f5d7410e176311e8e70b926af83fd01741f5dc40736a0986b0d2e6c543c94\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zkcfct22/wheels/f2/06/22/8c3eb86598b0d19253b87597aee575ca3d563d5ce2e4e79d8c\n",
            "Successfully built NeuroPredPLM\n",
            "Installing collected packages: fair-esm, einops, NeuroPredPLM\n",
            "Successfully installed NeuroPredPLM-0.1 einops-0.4.1 fair-esm-1.0.2\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/ISYSLAB-HUST/NeuroPred-PLM.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage\n",
        "- It takes a long time to download the parameter file when using it for the first time"
      ],
      "metadata": {
        "id": "ISAbT_qtGgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from NeuroPredPLM.predict import predict\n",
        "data = [\n",
        "    (\"peptide_1\", \"IGLRLPNMLKF\"),\n",
        "    (\"peptide_2\", \"QAAQFKVWSASELVD\"),\n",
        "    (\"peptide_3\",\"LRSPKMMHKSGCFGRRLDRIGSLSGLGCNVLRKY\")\n",
        "]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "neuropeptide_pred = predict(data,device)\n",
        "# {peptide_id:[Type:int(1->neuropeptide,0->non-neuropeptide), attention score:nd.array]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfQyQqnOD6w5",
        "outputId": "38cf477d-c6b8-44bd-99ac-2d32729a0a20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://zenodo.org/record/7042286/files/model.pth\" to /root/.cache/torch/hub/checkpoints/model.pth\n"
          ]
        }
      ]
    }
  ]
}